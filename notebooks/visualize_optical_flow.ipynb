{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.user_guided_dataset import UserGuidedVideoDataset\n",
    "import torch\n",
    "from skimage import color\n",
    "from math import round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame1 = '../datasets/bw-frames/all/01130.png'\n",
    "frame2 = '../datasets/bw-frames/all/01140.png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color1 = cv2.imread(frame1, cv2.IMREAD_COLOR)\n",
    "color2 = cv2.imread(frame2, cv2.IMREAD_COLOR)\n",
    "\n",
    "gray1 = cv2.cvtColor(color1, cv2.COLOR_BGR2GRAY)\n",
    "gray2 = cv2.cvtColor(color2, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "mask = np.zeros_like(color1)\n",
    "# Set image saturation to maximum value as we do not need it\n",
    "mask[..., 1] = 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow = cv2.calcOpticalFlowFarneback(gray1, gray2, None, pyr_scale = 0.5, levels = 5,\n",
    "                                    winsize = 11, iterations = 5, poly_n = 5, poly_sigma = 1.1, flags = 0)\n",
    "magnitude, angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
    "\n",
    "mask[..., 0] = angle * 180 / np.pi / 2\n",
    "# Set image value according to the optical flow magnitude (normalized)\n",
    "mask[..., 2] = cv2.normalize(magnitude, None, 0, 255, cv2.NORM_MINMAX)\n",
    "# Convert HSV to RGB (BGR) color representation\n",
    "rgb = cv2.cvtColor(mask, cv2.COLOR_HSV2BGR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dense_flow = cv2.addWeighted(color1, 1, rgb, 2, 0)\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(dense_flow)\n",
    "plt.figure()\n",
    "plt.imshow(color1)\n",
    "\n",
    "interval = 20\n",
    "for y in range(0, flow.shape[0], interval):\n",
    "    for x in range(0, flow.shape[1], interval):\n",
    "        plt.arrow(x, y, flow[y, x, 0], flow[y, x, 1], fc=\"k\", ec=\"k\", head_width=3, head_length=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample color patches and apply optical flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_channel, ab_channels, ab_hint, ab_mask, bounding_boxes = \\\n",
    "    UserGuidedVideoDataset('', False, [frame1], crop_to_fit=False)[0]\n",
    "replaced_ab = torch.where(torch.cat((ab_mask, ab_mask), dim=0) > 0, ab_hint, ab_channels)\n",
    "replaced_l = torch.where(ab_mask > 0, torch.ones_like(L_channel) * -0.5, L_channel)\n",
    "replaced_lab = torch.cat((replaced_l * 100 + 50, replaced_ab * 110), dim=0)\n",
    "replaced_rgb = color.lab2rgb(replaced_lab.permute((1, 2, 0)).numpy())\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(replaced_rgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shifted_ab_hint = torch.zeros_like(ab_hint)\n",
    "shifted_ab_mask = torch.zeros_like(ab_mask)\n",
    "\n",
    "hint_indices = ab_mask.nonzero()\n",
    "nonzero_y = hint_indices[:, 1]\n",
    "nonzero_x = hint_indices[:, 2]\n",
    "\n",
    "flow_in_CHW = torch.tensor(flow).permute((2, 0, 1))\n",
    "shifts_of_hint = flow_in_CHW[:, nonzero_y, nonzero_x]  # Values are in (delta_x, delta_y)!\n",
    "y_after_shift = torch.clamp(torch.round(shifts_of_hint[1] + nonzero_y).long(),\n",
    "                            0, ab_hint.shape[1] - 1)\n",
    "x_after_shift = torch.clamp(torch.round(shifts_of_hint[0] + nonzero_x).long(),\n",
    "                            0, ab_hint.shape[2] - 1)\n",
    "\n",
    "shifted_ab_mask[0, y_after_shift, x_after_shift] = 1\n",
    "shifted_ab_hint[:, y_after_shift, x_after_shift] = ab_hint[:, nonzero_y, nonzero_x]\n",
    "    \n",
    "img2_L_channel, img2_ab_channels, _, _, _ = \\\n",
    "    UserGuidedVideoDataset('', False, [frame2], crop_to_fit=False)[0]\n",
    "img2_replaced_ab = torch.where(torch.cat((shifted_ab_mask, shifted_ab_mask), dim=0) > 0,\n",
    "                               shifted_ab_hint, img2_ab_channels)\n",
    "img2_replaced_l = torch.where(shifted_ab_mask > 0, torch.ones_like(img2_L_channel) * -0.5, img2_L_channel)\n",
    "img2_replaced_lab = torch.cat((img2_replaced_l * 100 + 50, img2_replaced_ab * 110), dim=0)\n",
    "img2_replaced_rgb = color.lab2rgb(img2_replaced_lab.permute((1, 2, 0)).numpy())\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(img2_replaced_rgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
